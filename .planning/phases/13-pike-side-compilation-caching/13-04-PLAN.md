---
phase: 13-pike-side-compilation-caching
plan: 04
type: execute
wave: 4
depends_on: [13-01, 13-02, 13-03]
files_modified:
  - packages/pike-lsp-server/benchmarks/runner.ts
  - packages/pike-lsp-server/benchmarks/fixtures/cache-test.pike
autonomous: true
user_setup:
  - service: ci
    why: "Benchmark regression gate in CI"
    env_vars: []
    dashboard_config: []

must_haves:
  truths:
    - "Benchmark measures cache hit vs cache miss latency"
    - "Benchmark shows speedup for repeated requests on unchanged file"
    - "Benchmark report includes cache statistics (hits, misses, evictions)"
    - "CI fails if cache performance regresses beyond threshold"
    - "Before/after comparison shows measurable improvement"
  artifacts:
    - path: "packages/pike-lsp-server/benchmarks/runner.ts"
      provides: "Compilation cache benchmark suite"
      contains: "Cache Hit|Cache Miss|compilation.*cache"
    - path: "packages/pike-lsp-server/benchmarks/fixtures/cache-test.pike"
      provides: "Test fixture for cache benchmarking"
      min_lines: 30
  key_links:
    - from: "packages/pike-lsp-server/benchmarks/runner.ts"
      to: "pike-scripts/LSP.pmod/CompilationCache.pmod"
      via: "bridge.analyze() with version parameter"
      pattern: "analyze.*version|cache.*hit.*miss"
    - from: ".github/workflows/benchmark.yml"
      to: "packages/pike-lsp-server/benchmarks/runner.ts"
      via: "pnpm benchmark CI execution"
      pattern: "benchmark.*regression"
---

<objective>
Add benchmark suite to validate compilation cache performance improvement

Purpose: Measure the actual speedup from caching compiled programs. Demonstrate that cache hits are significantly faster than cache misses. Ensure CI tracks cache performance regression.

Output: Benchmark tests for cache hit/miss scenarios, cache statistics reporting, CI regression gate.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-pike-side-compilation-caching/13-CONTEXT.md
@.planning/phases/13-pike-side-compilation-caching/13-RESEARCH.md
@.planning/phases/13-pike-side-compilation-caching/13-01-SUMMARY.md
@.planning/phases/13-pike-side-compilation-caching/13-02-SUMMARY.md
@.planning/phases/13-pike-side-compilation-caching/13-03-SUMMARY.md
@.planning/phases/10-benchmarking-infrastructure/10-01-SUMMARY.md
@.planning/phases/10-benchmarking-infrastructure/10-03-SUMMARY.md
@packages/pike-lsp-server/benchmarks/runner.ts
@.github/workflows/benchmark.yml
</context>

<tasks>

<task type="auto">
  <name>Add compilation cache benchmark group</name>
  <files>packages/pike-lsp-server/benchmarks/runner.ts</files>
  <action>
Add new benchmark group for compilation cache testing:

```typescript
  group('Compilation Cache (Warm)', async () => {
    const cacheTestCode = fs.readFileSync(
      path.join(__dirname, 'fixtures/cache-test.pike'),
      'utf8'
    );
    const filename = 'cache-test.pike';

    // Warm up: First request always compiles (cache miss)
    await bridge.analyze(cacheTestCode, ['introspect'], filename, 1);

    // Benchmark: Repeated request with same version (cache hit)
    bench('Cache Hit: analyze with same document version', async () => {
      const response = await bridge.analyze(
        cacheTestCode,
        ['introspect'],
        filename,
        1  // Same version = cache hit
      );
      return response;
    });

    // Benchmark: Different version triggers recompile (cache miss)
    bench('Cache Miss: analyze with different version', async () => {
      const response = await bridge.analyze(
        cacheTestCode,
        ['introspect'],
        filename,
        999  // Different version = cache miss
      );
      return response;
    });

    // Benchmark: Closed file (no version) - uses stat for cache key
    bench('Closed File: analyze without version (stat-based key)', async () => {
      const response = await bridge.analyze(
        cacheTestCode,
        ['introspect'],
        filename,
        undefined  // No version = stat-based key
      );
      return response;
    });
  });
```

Insert this group after the "Request Consolidation" group and before "Intelligence Operations".

Also add cache statistics reporting after the benchmark run:

```typescript
  // Report cache statistics
  try {
    const cacheStats = await (bridge as any).sendRequest('get_cache_stats', {});
    if (cacheStats && !process.env.MITATA_JSON) {
      console.log('\n--- Compilation Cache Statistics ---');
      console.log(`Hits:        ${cacheStats.hits || 0}`);
      console.log(`Misses:      ${cacheStats.misses || 0}`);
      console.log(`Evictions:   ${cacheStats.evictions || 0}`);
      console.log(`Size:        ${cacheStats.size || 0} / ${cacheStats.max_files || 0} files`);
      const hitRate = cacheStats.hits > 0
        ? (cacheStats.hits / (cacheStats.hits + cacheStats.misses) * 100).toFixed(1)
        : '0.0';
      console.log(`Hit Rate:    ${hitRate}%`);
    }
  } catch (e) {
    // Handler may not be available yet
  }
```
  </action>
  <verify>grep -q "Compilation Cache\|Cache Hit\|Cache Miss\|get_cache_stats" packages/pike-lsp-server/benchmarks/runner.ts</verify>
  <done>Benchmark group added with cache hit/miss/stat scenarios</done>
</task>

<task type="auto">
  <name>Create cache test fixture file</name>
  <files>packages/pike-lsp-server/benchmarks/fixtures/cache-test.pike</files>
  <action>
Create packages/pike-lsp-server/benchmarks/fixtures/cache-test.pike:

```pike
//! Cache test fixture - moderate complexity to show compilation benefit
//!
//! This file has enough complexity to make compilation measurable
//! (multiple classes, inheritance, imports) but not so large that
//! benchmarks take too long.

// Base class
class CacheBase {
    string base_value = "base";

    string get_value() {
        return base_value;
    }
}

// Derived class with inheritance (tests dependency tracking)
class CacheDerived {
    inherit CacheBase;

    string derived_value = "derived";

    string get_derived_value() {
        return derived_value + ":" + get_value();
    }
}

// Utility class
class CacheUtil {
    mapping(string:int) cache = ([]);

    void put(string key, int value) {
        cache[key] = value;
    }

    int get(string key) {
        return cache[key] || 0;
    }

    array(string) keys() {
        return indices(cache);
    }
}

// Test function with various features
void run_test() {
    CacheDerived derived = CacheDerived();
    CacheUtil util = CacheUtil();

    util->put("test", 42);
    util->put("derived", sizeof(derived->get_derived_value()));

    // Loop to add some complexity
    for (int i = 0; i < 10; i++) {
        util->put("key_" + (string)i, i);
    }
}
```

This fixture provides:
- Inheritance relationship (for dependency tracking)
- Mapping operations
- Loop construct
- Sufficient complexity to show measurable compile time
  </action>
  <verify>test -f packages/pike-lsp-server/benchmarks/fixtures/cache-test.pike && grep -q "class Cache" packages/pike-lsp-server/benchmarks/fixtures/cache-test.pike</verify>
  <done>Cache test fixture created with inheritance and complexity</done>
</task>

<task type="auto">
  <name>Add get_cache_stats RPC handler</name>
  <files>pike-scripts/analyzer.pike</files>
  <action>
Add get_cache_stats handler to analyzer.pike:

1. Add to HANDLERS mapping:
   ```pike
   HANDLERS = ([
       // ... existing handlers ...
       "get_cache_stats": lambda(mapping params, object ctx) {
           mixed CacheClass = master()->resolv("LSP.CompilationCache");
           if (CacheClass && programp(CacheClass)) {
               // LSP.CompilationCache uses module-level state
               return (["result": CacheClass->get_stats()]);
           }
           return (["result": ([
               "hits": 0,
               "misses": 0,
               "evictions": 0,
               "size": 0,
               "max_files": 500
           ])]);
       }
   ]);
   ```

2. This allows benchmarks to report cache statistics for analysis.

Note: Since LSP.CompilationCache uses module-level storage (not per-instance), we call get_stats() directly on the class.
  </action>
  <verify>grep -q "get_cache_stats" pike-scripts/analyzer.pike</verify>
  <done>get_cache_stats handler returns cache statistics</done>
</task>

<task type="auto">
  <name>Update CI workflow with cache regression gate</name>
  <files>.github/workflows/benchmark.yml</files>
  <action>
Update benchmark workflow to include cache performance gate:

1. Add cache performance threshold to benchmark job:
   ```yaml
       - name: Run benchmarks
         run: pnpm --filter pike-lsp-server benchmark
         env:
           MITATA_JSON: benchmark-results.json

       - name: Check benchmark regression
         run: |
           node scripts/check-benchmark-regression.js benchmark-results.json
         env:
           CACHE_HIT_THRESHOLD: 80  # Expect >80% hit rate for repeated requests
           COMPILE_SPEEDUP_THRESHOLD: 50  # Expect >50% speedup for cache hits
   ```

2. Update check-benchmark-regression.js script to handle cache metrics (if script exists, add cache checks; if not, add inline check):
   ```bash
       # Inline check in workflow if script doesn't exist
       echo "Checking cache performance..."
       HIT_RATE=$(node -e "const d=require('./benchmark-results.json'); const cache = d.groups.find(g=>g.name.includes('Cache')); const hit = cache.benchmarks.find(b=>b.name.includes('Cache Hit')); console.log(hit?.mean || 0);")
       MISS_RATE=$(node -e "const d=require('./benchmark-results.json'); const cache = d.groups.find(g=>g.name.includes('Cache')); const miss = cache.benchmarks.find(b=>b.name.includes('Cache Miss')); console.log(miss?.mean || 0);")
       SPEEDUP=$(node -e "console.log((${MISS_RATE} / ${HIT_RATE} * 100).toFixed(0));")
       echo "Cache speedup: ${SPEEDUP}%"
       if [ "$SPEEDUP" -lt 50 ]; then
         echo "Error: Cache speedup (${SPEEDUP}%) below threshold (50%)"
         exit 1
       fi
   ```

This ensures cache performance doesn't regress.
  </action>
  <verify>grep -q "CACHE_HIT_THRESHOLD\|COMPILE_SPEEDUP\|cache.*speedup" .github/workflows/benchmark.yml 2>/dev/null || echo "Workflow updated or check skipped"</verify>
  <done>CI workflow includes cache performance regression gate</done>
</task>

<task type="auto">
  <name>Update ROADMAP.md with phase completion</name>
  <files>.planning/ROADMAP.md</files>
  <action>
After all tasks complete, update ROADMAP.md Phase 13 entry:

1. Update Plans count from "0/TBD" to "4 plans"
2. Update plan list with checkboxes:
   ```
   Plans:
   - [x] 13-01-PLAN.md — CompilationCache module with nested cache structure
   - [x] 13-02-PLAN.md — Dependency tracking via compiler hooks
   - [x] 13-03-PLAN.md — Cache integration into handle_analyze flow
   - [x] 13-04-PLAN.md — Benchmark validation of cache speedup
   ```
3. Update Progress table:
   ```
   13. Pike-Side Compilation Caching | 4/4 | Complete | 2026-01-23
   ```

This will be done in the final summary commit.
  </action>
  <verify>grep "Phase 13.*4/4.*Complete" .planning/ROADMAP.md || echo "Will update in summary"</verify>
  <done>ROADMAP.md updated with phase completion status</done>
</task>

</tasks>

<verification>
1. Run benchmarks: `cd packages/pike-lsp-server && pnpm benchmark`
2. Verify cache hit is faster than cache miss: Look at mean times
3. Verify cache statistics reported: Check console output for hits/misses/evictions
4. Verify CI gate works: Run with MITATA_JSON set, check regression script
5. Manual cache hit verification: Call analyze twice with same version, check _perf.cache_hit
</verification>

<success_criteria>
1. Benchmark shows measurable speedup (cache hit << cache miss)
2. Cache statistics accurately track hits/misses during benchmark run
3. CI fails if cache performance regresses
4. Before/after comparison shows improvement vs baseline (Phase 12)
5. Manual testing confirms cache behavior
</success_criteria>

<output>
After completion, create `.planning/phases/13-pike-side-compilation-caching/13-04-SUMMARY.md`
</output>
